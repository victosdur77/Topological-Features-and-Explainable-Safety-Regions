El repo del paper de XAI del CNR esta estructurado: Clonar el repo en una carpeta primero

-	El config.yaml que tiene la configuración de las simulaciones.
-	utilsData.py utilsPlot.py utils_SSVM.py y WrapScalableSVM.py y modelEvaluation.py algunas funciones definidas necesaias
-	El getdataset.py que hace la partición de los datos y los guarda en un csv.
-	SkopeRules.ipynb entrenamiento SKrules methods. SkopeAlgorithm.py es las fucnioens para entrenarlo
-	NativeXAI_performance.ipynb: Mide el rendimiento de las reglas de los métodos de reglas nativos como LLM y Skoperules
-	ConfidenceRegions_SVM.ipynb entrenamiento SVM para después aplicar anchors sobre las regiones de seguridad y ver rendimiento
-       AnchorGeneration.py para generar las anchors de los modelos mediante los dos métodos(Probabilistic scaling y conformal predictiosn
-	Anchor_CSR y Anchor_PSR.ipynb obtiene reglas tras haber aplicado CSR y PSR para obtener regiones de seguridad según el modelo.
-	AnchorAnalysis_PSR.ipynb y AnchorAnalysis_CSR.ipynb analiza las reglas obtenidas por cada métrica y su rendimiento

He modificado la manera de obtener los datos, de manera que no haga falta archivo .h5, mas rápido y sin problema. Bajando el numero de runs a 1000 y que se guarden la carpeta simulationVictor, también number de hilos=12. De esta manera unos 50 segundos en correr las 1000 simulaciones. Si ponemos 10000 simulaciones, como de forma original, unos 10 min. En esta parte es donde tendremos que hacer que el dataset calcule características topológicas.

He actualizado dos notebooks, el de SkopeRules.ipynb y el de utilsData ya que daba error algunas cosas.(Actualizado ya por Sara en el GitHub)

Si da error al importar six, o counter e iterable hay que cambiarlo. # in skope_rules.py, from sklearn.externals import six must be changed with import six. Also there, you have to change from collections import Counter, Iterable must be changed
# with from collections import Counter and from collections.abc import Iterable (dentro de los archivos del paquete de dicha dependencia se enceuntra ese skope_rules.py), sin esto el SkopeRules.ipynb no funciona.

Poner en save_rules y save_model = True en SkopeRules.ipynb para guardar las cosas y seguir con los siguientes notebook.

Dependencias necesarias: Python(3.10.12) navground(instalar desde Pypi), pandas(2.2.3), seaborn(0.13.2), scikit-learn(1.3.0), skope-rules(1.0.1), numpy(1.25.1, fundamental para que funcione), qpsolvers(pip install qpsolvers[open_source_solvers], cvxopt, anchor-exp(importante, cambia la versión de numpy, borrar numpy nuevo e instalar nuevamente 1.25.1). Python3.11 en el entorno.

Para ejecutar el NativeXAI_performance, hay que juntar los dos csv generados por Skoperules(para collision = 1 y no colision = 1). He creado una función que haga esto. Solo trabajamos con SkopeRules, no con llm. Se ha modificado el SkopeAlgorithm para que funcione bien y aadido la concatenación de archivos.

En Classic SVM training el solver osqp(por defecto) tarda 1 min y por ejemplo con el cvoxpt(con este tarda mucho unos 30 min). Documentación: https://pypi.org/project/qpsolvers/

A la hora de generar los anchors para ccsr y psr, el generateAnchorRULES Tarda en torno unos 35 minutos. 
